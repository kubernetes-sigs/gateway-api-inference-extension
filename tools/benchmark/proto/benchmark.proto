// Generate the proto:
// protoc --go_out=. --go_opt=paths=source_relative benchmark.proto

syntax = "proto3";
package benchmark.proto;

import "google/protobuf/timestamp.proto";

option go_package = "benchmark/proto";

// A wrapper to hold the global benchmark configuration. This is used to generate the Helm chart values.yaml
message Helm {
  Benchmark global = 1;
}

message Benchmarks {
  repeated Benchmark benchmarks = 1;
}

// Benchmark captures the information of a benchmark run, and will be persisted to the DB for data
// analysis.
message Benchmark {
  // Optional.
  BenchmarkCase benchmark_case = 1;
  // Required. User facing configuration to configure the benchmark manifests.
  BenchmarkConfig config = 2;
  // Optional. Result is automatically collected by the benchmark automation framework.
  BenchmarkResult result = 3;
  // Optional. Autogenerated by the tool.
  google.protobuf.Timestamp start_time = 4;
  // Optional. Name is used for matching the base_benchmark_name in the same Benchmarks config file.
  string name = 5;
  // Optional. The name of the parent benchmark configuration to base on.
  string base_benchmark_name = 6;
}

message BenchmarkCase {
  // Required.
  string name = 1;
  // Optional.
  string description = 2;
}

// BenchmarkConfig is the main user facing configuration for the benchmark run. It is used to
// generate benchmark manifests.
message BenchmarkConfig {
  // Required. Configuration about the load balancer.
  LoadBalancer load_balancer = 1;
  // Required. Configuration about the model server deployment.
  ModelServer model_server = 2;
  // Required. Configuration about the benchmark tooling.
  BenchmarkTool benchmark_tool = 3;
  // Optional.
  string namespace = 4;
}

message ModelServer {
  // Optional. Default: "vllm/vllm-openai:latest"
  string image = 1;
  // Optional. Type of the accelerator, e.g, nvidia-tesla-a100, nvidia-l4, etc.
  string accelerator = 2;
  // Optional. Default: 3
  int32 replicas = 3;
  oneof type {
    VLLM vllm = 4;
  }
}

message VLLM {
  // Optional. Default: "1"
  string tensor_parallelism = 1;
  // Optional. Default: "3"
  string max_loras = 2;
  // Optional. Default: "meta-llama/Llama-2-7b-hf"
  string model = 3;
  // Optional. Default: 16
  string lora_rank=4;
  // Optional. Default: "0".
  // If set to "1", the V1 model is used.
  string v1 = 5;
}

message BenchmarkTool {
  string image = 1; 
  oneof Type {
    LPG lpg = 2;
  }
}

message LPG {
  // Optional. Which input dataset to use, default to
  // ShareGPT_V3_unfiltered_cleaned_split.
  // Default: "sharegpt_v3_unfiltered_cleaned_split"
  string dataset = 2;
  // Optional. Which models to use, default to meta-llama/Llama-2-7b-hf
  // Default: "meta-llama/Llama-2-7b-hf"
  string models = 3;
  // Optional. Default: "model-server-service.benchmark-catalog.svc.cluster.local"
  string ip = 4;
  // Optional. Default: "8081"
  string port = 5;
  // Required.
  string request_rates = 6;
  // Optional. Default: "60"
  string benchmark_time_seconds = 7;
  // Optional. Default: "1024"
  string output_length = 8;
  string tokenizer = 9;
  string warmup_seconds = 10;
}

message LoadBalancer {
  oneof type {
    K8sService k8s_service = 1;
    Gateway gateway = 2;
  }

  // Define boolean flags to tell which type is enabled. This is due to a bug in Helm that you must
  // explicitly set whether a field is enabled or not. A non-existing field is considered enabled 
  // due to the bug https://github.com/helm/helm/issues/10296.
  bool k8s_service_enabled = 4;
  bool gateway_enabled = 5;
  bool gateway_envoy_enabled = 6;
  bool gateway_gke_gateway_enabled = 7;
  bool gateway_envoy_epp_enabled = 8;
  bool gateway_envoy_lb_policy_enabled = 9;
}

// By default the gateway name is `model-server-gateway` and port is `8081`.
message Gateway {
  oneof type {
    Envoy envoy = 1;
    GKEGateway gke_gateway = 2;
  }
  bool full_duplex_streaming_enabled = 3;
}

message Envoy {
  oneof type {
    EPP epp = 1;
    // Load balancing policies supported by Envoy: https://gateway.envoyproxy.io/docs/tasks/traffic/load-balancing/
    string lb_policy = 2;
  }
}
message GKEGateway{
  // Required.
  EPP epp = 1;
}

message EPP {
  // Optional. Default: "us-central1-docker.pkg.dev/k8s-staging-images/llm-instance-gateway/epp:main"
  string image = 1;
  // Optional. Default 50ms
  string refresh_metrics_interval = 2;
}

// By default the service is in the same namespace as the model server and lpg.
// The service name is `model-server-service` and port is `8081`.
message K8sService {
}

message BenchmarkResult {
  // Optional.
  repeated Stat stats = 1;
}


message Stat {
  // Optional.
  float request_rate = 1;
  // Optional.
  Metric request_latency = 2;
  // Optional.
  Metric throughput = 3;
  // Optional.
  Metric input_length = 4;
  // Optional.
  Metric output_length = 5;
  // Optional.
  Metric ttft = 6;
  // Optional.
  Metric tpot = 7;
  // Optional.
  repeated Metric model_server_metrics = 8;
}

message Metric {
  // Optional.
  string name = 1;
  // Optional.
  float mean = 2;
  // Optional.
  float median = 3;
  // Optional.
  float sd = 4;
  // Optional.
  float min = 5;
  // Optional.
  float max = 6;
  // Optional.
  float p90 = 7;
  // Optional.
  float p99 = 8;
}
# Note: If you change this file, please also change the file used for e2e tests!
# 
# https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/main/test/testdata/inferencepool-e2e.yaml

# --- ConfigMaps ---
apiVersion: v1
kind: ConfigMap
metadata:
  name: latency-predictor-config
  namespace: default
data:
  LATENCY_RETRAINING_INTERVAL_SEC: "1"
  LATENCY_MIN_SAMPLES_FOR_RETRAIN: "100"
  LATENCY_TTFT_MODEL_PATH: "/models/ttft.joblib"
  LATENCY_TPOT_MODEL_PATH: "/models/tpot.joblib"
  LATENCY_TTFT_SCALER_PATH: "/models/ttft_scaler.joblib"
  LATENCY_TPOT_SCALER_PATH: "/models/tpot_scaler.joblib"
  LATENCY_MODEL_TYPE: "xgboost"
  LATENCY_MAX_TRAINING_DATA_SIZE_PER_BUCKET: "5000"
  LATENCY_QUANTILE_ALPHA: "0.9"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prediction-server-config
  namespace: default
data:
  LATENCY_MODEL_TYPE: "xgboost"
  PREDICT_HOST: "0.0.0.0"
  LOCAL_TTFT_MODEL_PATH: "/server_models/ttft.joblib"  # Use individual storage
  LOCAL_TPOT_MODEL_PATH: "/server_models/tpot.joblib"
  LOCAL_TTFT_SCALER_PATH: "/server_models/ttft_scaler.joblib"
  LOCAL_TPOT_SCALER_PATH: "/server_models/tpot_scaler.joblib"
---
# --- InferencePool ---
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: vllm-llama3-8b-instruct
spec:
  targetPortNumber: 8000
  selector:
    app: vllm-llama3-8b-instruct
  extensionRef:
    name: vllm-llama3-8b-instruct-epp
---
# --- EPP Service ---
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama3-8b-instruct-epp
  namespace: default
spec:
  selector:
    app: vllm-llama3-8b-instruct-epp
  ports:
    - name: epp-grpc
      protocol: TCP
      port: 9002
      targetPort: 9002
      appProtocol: http2
    - name: latency-predictor-training
      protocol: TCP
      port: 8000
      targetPort: 8000 
    - name: latency-predictor-1
      protocol: TCP
      port: 8001
      targetPort: 8001  
    - name: latency-predictor-2
      protocol: TCP
      port: 8002
      targetPort: 8002
    - name: latency-predictor-3
      protocol: TCP
      port: 8003
      targetPort: 8003
    - name: latency-predictor-4
      protocol: TCP
      port: 8004
      targetPort: 8004
    - name: latency-predictor-5
      protocol: TCP
      port: 8005
      targetPort: 8005
    - name: latency-predictor-6
      protocol: TCP
      port: 8006
      targetPort: 8006
    - name: latency-predictor-7
      protocol: TCP
      port: 8007
      targetPort: 8007
    - name: latency-predictor-8
      protocol: TCP
      port: 8008
      targetPort: 8008
    - name: latency-predictor-9
      protocol: TCP
      port: 8009
      targetPort: 8009
    - name: latency-predictor-10
      protocol: TCP
      port: 8010
      targetPort: 8010
    - name: prometheus
      protocol: TCP
      port: 9090
      targetPort: 9090
  type: LoadBalancer 
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vllm-llama3-8b-instruct-epp
  namespace: default
---
# --- EPP Deployment with Individual Container Volumes ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-8b-instruct-epp
  namespace: default
  labels:
    app: vllm-llama3-8b-instruct-epp
spec:
  replicas: 1  # Multiple EPP pods for scaling
  selector:
    matchLabels:
      app: vllm-llama3-8b-instruct-epp
  template:
    metadata:
      labels:
        app: vllm-llama3-8b-instruct-epp
    spec:
      serviceAccountName: vllm-llama3-8b-instruct-epp
      # Conservatively, this timeout should mirror the longest grace period of the pods within the pool
      containers:
      # EPP Container
      - name: epp
        image:  us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/slo-routing-epp-exp
        imagePullPolicy: Always
        args:
        - -pool-name
        - "vllm-llama3-8b-instruct"
        - "-pool-namespace"
        - "default"
        - --pool-group
        - "inference.networking.x-k8s.io"
        - -v
        - "4"
        - --zap-encoder
        - "json"
        - -grpc-port
        - "9002"
        - -grpc-health-port
        - "9003"
        - "--config-file"
        - "/config/default-plugins.yaml"
        - "-enable-latency-predictor"
        env:
        - name: PREDICTION_SERVER_URL
          value: "http://localhost:8001,http://localhost:8002,http://localhost:8003,http://localhost:8004,http://localhost:8005,http://localhost:8006,http://localhost:8007,http://localhost:8008,http://localhost:8009,http://localhost:8010"  # All 10 prediction servers
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"  # Single training server for sending training data
        - name: LATENCY_MAX_SAMPLE_SIZE
          value: "10000"  # Maximum sample size for latency prediction



        
        ports:
        - containerPort: 9002
        - containerPort: 9003
        - name: metrics
          containerPort: 9090
        livenessProbe:
          grpc:
            port: 9003
            service: inference-extension
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          grpc:
            port: 9003
            service: inference-extension
          initialDelaySeconds: 5
          periodSeconds: 10
        volumeMounts:
        - name: plugins-config-volume
          mountPath: "/config"
      # Training Server Sidecar Container
      - name: training-server
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_training:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8000
          name: training-port
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8000
          initialDelaySeconds: 45
          periodSeconds: 10
        resources:
          requests:
            cpu: "2000m"
            memory: "4Gi"
          limits:
            cpu: "4000m"
            memory: "8Gi"
        envFrom:
        - configMapRef:
            name: latency-predictor-config
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "training"
        volumeMounts:
        - name: training-server-storage
          mountPath: /models
      # Prediction Server Sidecar Container 1
      - name: prediction-server-1
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8001"]
        ports:
        - containerPort: 8001
          name: predict-port-1
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8001
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8001
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8001"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-1"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-1-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 2
      - name: prediction-server-2
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8002"]
        ports:
        - containerPort: 8002
          name: predict-port-2
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8002
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8002
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8002"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-2"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-2-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 3
      - name: prediction-server-3
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8003"]
        ports:
        - containerPort: 8003
          name: predict-port-3
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8003
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8003
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8003"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-3"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-3-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 4
      - name: prediction-server-4
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8004"]
        ports:
        - containerPort: 8004
          name: predict-port-4
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8004
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8004
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8004"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-4"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-4-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 5
      - name: prediction-server-5
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8005"]
        ports:
        - containerPort: 8005
          name: predict-port-5
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8005
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8005
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8005"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-5"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-5-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 6
      - name: prediction-server-6
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8006"]
        ports:
        - containerPort: 8006
          name: predict-port-6
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8006
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8006
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8006"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-6"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-6-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 7
      - name: prediction-server-7
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8007"]
        ports:
        - containerPort: 8007
          name: predict-port-7
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8007
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8007
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8007"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-7"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-7-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 8
      - name: prediction-server-8
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8008"]
        ports:
        - containerPort: 8008
          name: predict-port-8
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8008
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8008
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8008"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-8"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-8-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 9
      - name: prediction-server-9
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8009"]
        ports:
        - containerPort: 8009
          name: predict-port-9
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8009
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8009
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8009"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-9"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-9-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 10
      - name: prediction-server-10
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8010"]
        ports:
        - containerPort: 8010
          name: predict-port-10
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8010
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8010
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8010"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-10"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-10-storage
          mountPath: /server_models
      volumes:
      - name: training-server-storage
        emptyDir: 
          sizeLimit: "20Gi"  # Dedicated volume for training server
      - name: prediction-server-1-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 1
      - name: prediction-server-2-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 2
      - name: prediction-server-3-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 3
      - name: prediction-server-4-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 4
      - name: prediction-server-5-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 5
      - name: prediction-server-6-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 6
      - name: prediction-server-7-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 7
      - name: prediction-server-8-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 8
      - name: prediction-server-9-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 9
      - name: prediction-server-10-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 10
      - name: plugins-config-volume
        configMap:
          name: plugins-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: plugins-config
  namespace: default
data:
  default-plugins.yaml: |
    apiVersion: inference.networking.x-k8s.io/v1alpha1
    kind: EndpointPickerConfig
    plugins:
    - type: queue-scorer
    - type: kv-cache-utilization-scorer
    - type: prefix-cache-scorer
    - type: slo-aware-routing
    - type: slo-aware-profile-handler
    - type: max-score-picker
    schedulingProfiles:
    - name: prefix
      plugins:
      - pluginRef: prefix-cache-scorer
    - name: default
      plugins:
      - pluginRef: slo-aware-routing
        weight: 0
      - pluginRef: queue-scorer
      - pluginRef: kv-cache-utilization-scorer
      - pluginRef: max-score-picker
    - name: slo
      plugins:
      - pluginRef: slo-aware-routing
      - pluginRef: max-score-picker

---
# --- RBAC ---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pod-read
  namespace: default
rules:
- apiGroups: [ "inference.networking.x-k8s.io" ]
  resources: [ "inferenceobjectives", "inferencepools" ]
  verbs: [ "get", "watch", "list" ]
- apiGroups: [ "inference.networking.k8s.io" ]
  resources: [ "inferencepools" ]
  verbs: [ "get", "watch", "list" ]
- apiGroups: [ "" ]
  resources: [ "pods" ]
  verbs: [ "get", "watch", "list" ]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pod-read-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: vllm-llama3-8b-instruct-epp
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-read
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: auth-reviewer
rules:
- apiGroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apiGroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: auth-reviewer-binding
subjects:
- kind: ServiceAccount
  name: vllm-llama3-8b-instruct-epp
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: auth-reviewer
inferenceExtension:
  # Number of replicas
  replicas: 1
  image:
    name: epp
    hub: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension
    tag: main
    pullPolicy: Always
  extProcPort: 9002
  env: []
  enablePprof: true  # Enable pprof handlers for profiling and debugging
  modelServerMetricsPath: "/metrics"
  modelServerMetricsScheme: "http"
  modelServerMetricsHttpsInsecureSkipVerify: true
  grpcPort: 9002
  grpcHealthPort: 9003
  metricsPort: 9090
  poolName: ""
  poolNamespace: "default"
  refreshMetricsInterval: "50ms"
  refreshPrometheusMetricsInterval: "5s"
  secureServing: true
  healthChecking: false
  totalQueuedRequestsMetric: "vllm:num_requests_waiting"
  kvCacheUsagePercentageMetric: "vllm:gpu_cache_usage_perc"
  loraInfoMetric: "vllm:lora_requests_info"
  certPath: ""
  metricsStalenessThreshold: "2s"
  
  pluginsConfigFile: "default-plugins.yaml"
  logVerbosity: 1

  eppFlags:
    - name: flag1
      value: "value1"

  # Define additional container ports
  modelServerMetricsPort: 0
  extraContainerPorts: []
  # Define additional service ports
  extraServicePorts: []

inferencePool:
  targetPortNumber: 8000
  modelServerType: vllm # vllm, triton-tensorrt-llm
  modelServers:  # REQUIRED
    matchLabels:
      app: vllm-llama3-8b-instruct

provider:
  name: none

gke:
  monitoringSecret:
    name: inference-gateway-sa-metrics-reader-secret
    namespace: default

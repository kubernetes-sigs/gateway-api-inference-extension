apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "gateway-api-inference-extension.name" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "gateway-api-inference-extension.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.inferenceExtension.replicas | default 1 }}
  selector:
    matchLabels:
      {{- include "gateway-api-inference-extension.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "gateway-api-inference-extension.selectorLabels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "gateway-api-inference-extension.name" . }}
      # Conservatively, this timeout should mirror the longest grace period of the pods within the pool
      terminationGracePeriodSeconds: 130
      containers:
      - name: epp
        image: {{ .Values.inferenceExtension.image.hub }}/{{ .Values.inferenceExtension.image.name }}:{{ .Values.inferenceExtension.image.tag }}
        imagePullPolicy: {{ .Values.inferenceExtension.image.pullPolicy | default "Always" }}
        args:
        - --pool-name
        - {{ .Release.Name }}
        - --pool-namespace
        - {{ .Release.Namespace }}
        - --zap-encoder
        - "json"
        - --config-file
        - "/config/{{ .Values.inferenceExtension.pluginsConfigFile }}"
        {{- if ne .Values.inferencePool.apiVersion "inference.networking.k8s.io/v1" }}
        - --pool-group
        - "{{ (split "/" .Values.inferencePool.apiVersion)._0 }}"
        {{- end }}
        {{- range .Values.inferenceExtension.flags }}
        - "--{{ .name }}"
        - "{{ .value }}"
        {{- end }}
        {{- if eq (.Values.inferencePool.modelServerType | default "vllm") "triton-tensorrt-llm" }}
        - --total-queued-requests-metric
        - "nv_trt_llm_request_metrics{request_type=waiting}"
        - --kv-cache-usage-percentage-metric
        - "nv_trt_llm_kv_cache_block_metrics{kv_cache_block_type=fraction}"
        - --lora-info-metric
        - "" # Set an empty metric to disable LoRA metric scraping as they are not supported by Triton yet.
        {{- end }}
        ports:
        - name: grpc
          containerPort: 9002
        - name: grpc-health
          containerPort: 9003
        - name: metrics
          containerPort: 9090
        {{- if .Values.inferenceExtension.extraContainerPorts }}
        {{- toYaml .Values.inferenceExtension.extraContainerPorts | nindent 8 }}
        {{- end }}
        livenessProbe:
          {{- if .Values.inferenceExtension.enableLeaderElection }}
          grpc:
            port: 9003
            service: liveness
          {{- else }}
          grpc:
            port: 9003
            service: inference-extension
          {{- end }}
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          {{- if .Values.inferenceExtension.enableLeaderElection }}
          grpc:
            port: 9003
            service: readiness
          {{- else }}
          grpc:
            port: 9003
            service: inference-extension
          {{- end }}
          initialDelaySeconds: 5
          periodSeconds: 10
        {{- if .Values.inferenceExtension.env }}
        env:
        {{- toYaml .Values.inferenceExtension.env | nindent 8 }}
        {{- end }}
        volumeMounts:
        - name: plugins-config-volume
          mountPath: "/config"
      {{- if .Values.inferenceExtension.sidecars }}
      {{- if .Values.inferenceExtension.sidecars.trainingServer }}
      {{- if .Values.inferenceExtension.sidecars.trainingServer.enabled }}
      # Training Server Sidecar Container
      - name: training-server
        image: {{ .Values.inferenceExtension.sidecars.trainingServer.image.hub }}/{{ .Values.inferenceExtension.sidecars.trainingServer.image.name }}:{{ .Values.inferenceExtension.sidecars.trainingServer.image.tag }}
        imagePullPolicy: {{ .Values.inferenceExtension.sidecars.trainingServer.image.pullPolicy | default "Always" }}
        ports:
        - containerPort: 8000
          name: training-port
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8000
          initialDelaySeconds: 45
          periodSeconds: 10
        {{- if .Values.inferenceExtension.sidecars.trainingServer.resources }}
        resources:
          {{- toYaml .Values.inferenceExtension.sidecars.trainingServer.resources | nindent 10 }}
        {{- end }}
        envFrom:
        {{- if .Values.inferenceExtension.sidecars.trainingServer.envFrom }}
          {{- toYaml .Values.inferenceExtension.sidecars.trainingServer.envFrom | nindent 10 }}
        {{- else }}
        - configMapRef:
            name: latency-predictor-config
        {{- end }}
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "training"
        {{- if .Values.inferenceExtension.sidecars.trainingServer.env }}
        {{- toYaml .Values.inferenceExtension.sidecars.trainingServer.env | nindent 8 }}
        {{- end }}
        volumeMounts:
        - name: training-server-storage
          mountPath: /models
      {{- end }}
      {{- end }}
      {{- if .Values.inferenceExtension.sidecars.predictionServers }}
      {{- if .Values.inferenceExtension.sidecars.predictionServers.enabled }}
      {{- $replicas := int (.Values.inferenceExtension.sidecars.predictionServers.replicas | default 3) }}
      {{- range $i := until $replicas }}
      # Prediction Server Sidecar Container {{ add $i 1 }}
      - name: prediction-server-{{ add $i 1 }}
        image: {{ $.Values.inferenceExtension.sidecars.predictionServers.image.hub }}/{{ $.Values.inferenceExtension.sidecars.predictionServers.image.name }}:{{ $.Values.inferenceExtension.sidecars.predictionServers.image.tag }}
        imagePullPolicy: {{ $.Values.inferenceExtension.sidecars.predictionServers.image.pullPolicy | default "Always" }}
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "{{ add 8001 $i }}"]
        ports:
        - containerPort: {{ add 8001 $i }}
          name: predict-port-{{ add $i 1 }}
        livenessProbe:
          httpGet:
            path: /healthz
            port: {{ add 8001 $i }}
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: {{ add 8001 $i }}
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        {{- if $.Values.inferenceExtension.sidecars.predictionServers.resources }}
        resources:
          {{- toYaml $.Values.inferenceExtension.sidecars.predictionServers.resources | nindent 10 }}
        {{- end }}
        envFrom:
        {{- if $.Values.inferenceExtension.sidecars.predictionServers.envFrom }}
          {{- toYaml $.Values.inferenceExtension.sidecars.predictionServers.envFrom | nindent 10 }}
        {{- else }}
        - configMapRef:
            name: prediction-server-config
        {{- end }}
        env:
        - name: PREDICT_PORT
          value: "{{ add 8001 $i }}"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-{{ add $i 1 }}"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        {{- if $.Values.inferenceExtension.sidecars.predictionServers.env }}
        {{- toYaml $.Values.inferenceExtension.sidecars.predictionServers.env | nindent 8 }}
        {{- end }}
        volumeMounts:
        - name: prediction-server-{{ add $i 1 }}-storage
          mountPath: /server_models
      {{- end }}
      {{- end }}
      {{- end }}
      {{- end }}
      volumes:
      - name: plugins-config-volume
        configMap:
          name: {{ include "gateway-api-inference-extension.name" . }}
      {{- if .Values.inferenceExtension.sidecars }}
      {{- if .Values.inferenceExtension.sidecars.trainingServer }}
      {{- if .Values.inferenceExtension.sidecars.trainingServer.enabled }}
      - name: training-server-storage
        {{- if .Values.inferenceExtension.sidecars.trainingServer.persistence }}
        {{- if .Values.inferenceExtension.sidecars.trainingServer.persistence.enabled }}
        persistentVolumeClaim:
          claimName: {{ .Values.inferenceExtension.sidecars.trainingServer.persistence.claimName | default "training-models-pvc" }}
        {{- else }}
        emptyDir: {}
        {{- end }}
        {{- else }}
        emptyDir: {}
        {{- end }}
      {{- end }}
      {{- end }}
      {{- if .Values.inferenceExtension.sidecars.predictionServers }}
      {{- if .Values.inferenceExtension.sidecars.predictionServers.enabled }}
      {{- $replicas := int (.Values.inferenceExtension.sidecars.predictionServers.replicas | default 3) }}
      {{- range $i := until $replicas }}
      - name: prediction-server-{{ add $i 1 }}-storage
        emptyDir: {}
      {{- end }}
      {{- end }}
      {{- end }}
      {{- end }}
      {{- if .Values.inferenceExtension.affinity }}
      affinity:
        {{- toYaml .Values.inferenceExtension.affinity | nindent 8 }}
      {{- end }}
      {{- if .Values.inferenceExtension.tolerations }}
      tolerations:
        {{- toYaml .Values.inferenceExtension.tolerations | nindent 8 }}
      {{- end }}

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "gateway-api-inference-extension.name" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "gateway-api-inference-extension.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.inferenceExtension.replicas | default 1 }}
  strategy:
    # The current recommended EPP deployment pattern is to have a single active replica. This ensures 
    # optimal performance of the stateful operations such prefix cache aware scorer.
    # The Recreate strategy the old replica is killed immediately, and allow the new replica(s) to 
    # quickly take over. This is particularly important in the high availability set up with leader
    # election, as the rolling update strategy would prevent the old leader being killed because 
    # otherwise the maxUnavailable would be 100%.
    type: Recreate
  selector:
    matchLabels:
      {{- include "gateway-api-inference-extension.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "gateway-api-inference-extension.selectorLabels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "gateway-api-inference-extension.name" . }}
      # Conservatively, this timeout should mirror the longest grace period of the pods within the pool
      terminationGracePeriodSeconds: 130
      containers:
      {{- if .Values.inferenceExtension.sidecar.enabled }}
      - name: {{ .Values.inferenceExtension.sidecar.name }}
        image: {{ .Values.inferenceExtension.sidecar.image }}
        imagePullPolicy: {{ .Values.inferenceExtension.sidecar.image.pullPolicy | default "IfNotPresent" }}
        {{- with .Values.inferenceExtension.sidecar.command }}
        command:
          - {{ . | quote }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.args }}
        args:
          {{- range . }}
            - {{ tpl . $ | quote }}
          {{- end }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.env }}
        env:
          {{- toYaml . | nindent 10 }}
          {{- end }}
        {{- with .Values.inferenceExtension.sidecar.ports }}
        ports:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.livenessProbe }}
        livenessProbe:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.readinessProbe }}
        readinessProbe:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.resources }}
        resources:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- with .Values.inferenceExtension.sidecar.volumeMounts }}
        volumeMounts:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- end }}
      - name: epp
        image: {{ .Values.inferenceExtension.image.hub }}/{{ .Values.inferenceExtension.image.name }}:{{ .Values.inferenceExtension.image.tag }}
        imagePullPolicy: {{ .Values.inferenceExtension.image.pullPolicy | default "IfNotPresent" }}
        args:
        - --pool-name
        - {{ .Release.Name }}
        # The pool namespace is optional because EPP can default to the NAMESPACE env var.
        # We still keep this here so that the template works with older versions of EPP, or other
        # distros of EPP which may not have implemented the NAMESPACE env var defaulting behavior.
        - --pool-namespace
        - {{ .Release.Namespace }}
        {{- if ne .Values.inferencePool.apiVersion "inference.networking.k8s.io" }}
        - --pool-group
        - "{{ (split "/" .Values.inferencePool.apiVersion)._0 }}"
        {{- end }}
        - --zap-encoder
        - "json"
        - --config-file
        - "/config/{{ .Values.inferenceExtension.pluginsConfigFile }}"
        {{- if eq (.Values.inferencePool.modelServerType | default "vllm") "triton-tensorrt-llm" }}
        - --total-queued-requests-metric
        - "nv_trt_llm_request_metrics{request_type=waiting}"
        - --kv-cache-usage-percentage-metric
        - "nv_trt_llm_kv_cache_block_metrics{kv_cache_block_type=fraction}"
        - --lora-info-metric
        - "" # Set an empty metric to disable LoRA metric scraping as they are not supported by Triton yet.
        {{- end }}
        {{- if gt (.Values.inferenceExtension.replicas | int) 1 }}
        - --ha-enable-leader-election
        {{- end }}
        {{- if .Values.inferenceExtension.latencyPredictor.enabled }}
        - --enable-latency-predictor
        {{- end }}
        # Pass additional flags via the inferenceExtension.flags field in values.yaml.
        {{- range $key, $value := .Values.inferenceExtension.flags }}
        - --{{ $key }}
        - "{{ $value }}"
        {{- end }}
        {{- if .Values.inferenceExtension.tracing.enabled }}
        - --tracing=true
        {{- else }}
        - --tracing=false
        {{- end }}
        {{- if not .Values.inferenceExtension.monitoring.prometheus.enabled }}
        - --metrics-endpoint-auth=false
        {{- end }}
        ports:
        - name: grpc
          containerPort: 9002
        - name: grpc-health
          containerPort: 9003
        - name: metrics
          containerPort: 9090
        {{- if .Values.inferenceExtension.extraContainerPorts }}
        {{- toYaml .Values.inferenceExtension.extraContainerPorts | nindent 8 }}
        {{- end }}
        livenessProbe:
          {{- if gt (.Values.inferenceExtension.replicas | int) 1 }}
          grpc:
            port: 9003
            service: liveness
          {{- else }}
          grpc:
            port: 9003
            service: inference-extension
          {{- end }}
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          {{- if gt (.Values.inferenceExtension.replicas | int) 1 }}
          grpc:
            port: 9003
            service: readiness
          {{- else }}
          grpc:
            port: 9003
            service: inference-extension
          {{- end }}
          periodSeconds: 2
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        {{- if .Values.inferenceExtension.latencyPredictor.enabled }}
        - name: PREDICTION_SERVER_URL
          value: "{{- $count := int .Values.inferenceExtension.latencyPredictor.predictionServers.count -}}
                  {{- $startPort := int .Values.inferenceExtension.latencyPredictor.predictionServers.startPort -}}
                  {{- range $i := until $count -}}
                    {{- if $i }},{{ end }}http://localhost:{{ add $startPort $i }}
                  {{- end }}"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:{{ .Values.inferenceExtension.latencyPredictor.trainingServer.port }}"
        {{- range $key, $value := .Values.inferenceExtension.latencyPredictor.eppEnv }}
        - name: {{ $key }}
          value: {{ $value | quote }}
        {{- end }}
        {{- end }}
        {{- if .Values.inferenceExtension.tracing.enabled }}
        - name: OTEL_SERVICE_NAME
          value: "gateway-api-inference-extension"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: {{ .Values.inferenceExtension.tracing.otelExporterEndpoint | quote }}
        - name: OTEL_TRACES_EXPORTER
          value: "otlp"
        - name: OTEL_RESOURCE_ATTRIBUTES_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: OTEL_RESOURCE_ATTRIBUTES_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: 'k8s.namespace.name=$(NAMESPACE),k8s.node.name=$(OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=$(OTEL_RESOURCE_ATTRIBUTES_POD_NAME)'
        - name: OTEL_TRACES_SAMPLER
          value: {{ .Values.inferenceExtension.tracing.sampling.sampler | quote }}
        - name: OTEL_TRACES_SAMPLER_ARG
          value: {{ .Values.inferenceExtension.tracing.sampling.samplerArg | quote }}
        {{- end }}
        {{- if .Values.inferenceExtension.env }}
        {{- toYaml .Values.inferenceExtension.env | nindent 8 }}
        {{- end }}
        volumeMounts:
        - name: plugins-config-volume
          mountPath: "/config"
      {{- if .Values.inferenceExtension.latencyPredictor.enabled }}
      # Training Server Sidecar Container
      - name: training-server
        image: {{ .Values.inferenceExtension.latencyPredictor.trainingServer.image.hub }}/{{ .Values.inferenceExtension.latencyPredictor.trainingServer.image.name }}:{{ .Values.inferenceExtension.latencyPredictor.trainingServer.image.tag }}
        imagePullPolicy: {{ .Values.inferenceExtension.latencyPredictor.trainingServer.image.pullPolicy }}
        ports:
        - containerPort: {{ .Values.inferenceExtension.latencyPredictor.trainingServer.port }}
          name: training-port
        livenessProbe:
          {{- toYaml .Values.inferenceExtension.latencyPredictor.trainingServer.livenessProbe | nindent 10 }}
        readinessProbe:
          {{- toYaml .Values.inferenceExtension.latencyPredictor.trainingServer.readinessProbe | nindent 10 }}
        resources:
          {{- toYaml .Values.inferenceExtension.latencyPredictor.trainingServer.resources | nindent 10 }}
        envFrom:
        - configMapRef:
            name: {{ include "gateway-api-inference-extension.name" . }}-latency-predictor-training
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "training"
        volumeMounts:
        - name: training-server-storage
          mountPath: /models
      {{- range $i := until (int .Values.inferenceExtension.latencyPredictor.predictionServers.count) }}
      # Prediction Server Sidecar Container {{ add $i 1 }}
      - name: prediction-server-{{ add $i 1 }}
        image: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.image.hub }}/{{ $.Values.inferenceExtension.latencyPredictor.predictionServers.image.name }}:{{ $.Values.inferenceExtension.latencyPredictor.predictionServers.image.tag }}
        imagePullPolicy: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.image.pullPolicy }}
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "{{ add $.Values.inferenceExtension.latencyPredictor.predictionServers.startPort $i }}"]
        ports:
        - containerPort: {{ add $.Values.inferenceExtension.latencyPredictor.predictionServers.startPort $i }}
          name: predict-port-{{ add $i 1 }}
        livenessProbe:
          httpGet:
            path: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.livenessProbe.httpGet.path }}
            port: {{ add $.Values.inferenceExtension.latencyPredictor.predictionServers.startPort $i }}
          initialDelaySeconds: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.livenessProbe.initialDelaySeconds }}
          periodSeconds: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.livenessProbe.periodSeconds }}
        readinessProbe:
          httpGet:
            path: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.readinessProbe.httpGet.path }}
            port: {{ add $.Values.inferenceExtension.latencyPredictor.predictionServers.startPort $i }}
          initialDelaySeconds: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.readinessProbe.initialDelaySeconds }}
          periodSeconds: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.readinessProbe.periodSeconds }}
          failureThreshold: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.readinessProbe.failureThreshold }}
        resources:
          {{- toYaml $.Values.inferenceExtension.latencyPredictor.predictionServers.resources | nindent 10 }}
        envFrom:
        - configMapRef:
            name: {{ include "gateway-api-inference-extension.name" $ }}-latency-predictor-prediction
        env:
        - name: PREDICT_PORT
          value: "{{ add $.Values.inferenceExtension.latencyPredictor.predictionServers.startPort $i }}"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-{{ add $i 1 }}"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:{{ $.Values.inferenceExtension.latencyPredictor.trainingServer.port }}"
        volumeMounts:
        - name: prediction-server-{{ add $i 1 }}-storage
          mountPath: /server_models
      {{- end }}
      {{- end }}
      volumes:
      {{- if .Values.inferenceExtension.sidecar.volumes }}
      {{- tpl (toYaml .Values.inferenceExtension.sidecar.volumes) $ | nindent 6 }}
      {{- end }}
      - name: plugins-config-volume
        configMap:
          name: {{ include "gateway-api-inference-extension.name" . }}
      {{- if .Values.inferenceExtension.latencyPredictor.enabled }}
      - name: training-server-storage
        emptyDir: 
          sizeLimit: {{ .Values.inferenceExtension.latencyPredictor.trainingServer.volumeSize }}
      {{- range $i := until (int .Values.inferenceExtension.latencyPredictor.predictionServers.count) }}
      - name: prediction-server-{{ add $i 1 }}-storage
        emptyDir: 
          sizeLimit: {{ $.Values.inferenceExtension.latencyPredictor.predictionServers.volumeSize }}
      {{- end }}
      {{- end }}
      {{- if .Values.inferenceExtension.affinity }}
      affinity:
        {{- toYaml .Values.inferenceExtension.affinity | nindent 8 }}
      {{- end }}
      {{- if .Values.inferenceExtension.tolerations }}
      tolerations:
        {{- toYaml .Values.inferenceExtension.tolerations | nindent 8 }}
      {{- end }}

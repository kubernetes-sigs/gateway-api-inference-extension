inferenceExtension:
  replicas: 1
  image:
    name: epp
    hub: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension
    tag: main
    pullPolicy: Always
  extProcPort: 9002
  env: []
  pluginsConfigFile: "default-plugins.yaml"
  # Define additional container ports
  extraContainerPorts: []
  # Define additional service ports
  extraServicePorts: []

  # This is the plugins configuration file.
  # pluginsCustomConfig:
  #   custom-plugins.yaml: |
  #     apiVersion: inference.networking.x-k8s.io/v1alpha1
  #     kind: EndpointPickerConfig
  #     plugins:
  #     - type: custom-scorer
  #       parameters:
  #         custom-threshold: 64
  #     schedulingProfiles:
  #     - name: default
  #       plugins:
  #       - pluginRef: custom-scorer

  # Example environment variables:
  # env:
  #   ENABLE_EXPERIMENTAL_FEATURE: "true"

  flags:
    # Log verbosity
    - name: v
      value: 1

  affinity: {}

  tolerations: []

  # SLO-aware routing with latency prediction sidecars
  # Uncomment and configure to enable SLO prediction
  # sidecars:
  #   trainingServer:
  #     enabled: false
  #     image:
  #       hub: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension
  #       name: latency-training
  #       tag: latest
  #       pullPolicy: Always
  #     resources:
  #       requests:
  #         cpu: "2000m"
  #         memory: "4Gi"
  #       limits:
  #         cpu: "4000m"
  #         memory: "8Gi"
  #     config:
  #       retrainingIntervalSec: "1"
  #       minSamplesForRetrain: "100"
  #       ttftModelPath: "/models/ttft.joblib"
  #       tpotModelPath: "/models/tpot.joblib"
  #       ttftScalerPath: "/models/ttft_scaler.joblib"
  #       tpotScalerPath: "/models/tpot_scaler.joblib"
  #       modelType: "xgboost"
  #     persistence:
  #       enabled: false
  #       claimName: "training-models-pvc"
  #     env: []
  #     envFrom: []
  #
  #   predictionServers:
  #     enabled: false
  #     replicas: 3
  #     image:
  #       hub: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension
  #       name: latency-prediction
  #       tag: latest
  #       pullPolicy: Always
  #     resources:
  #       requests:
  #         cpu: "500m"
  #         memory: "1Gi"
  #       limits:
  #         cpu: "1000m"
  #         memory: "2Gi"
  #     config:
  #       modelSyncIntervalSec: "10"
  #       modelType: "xgboost"
  #       host: "0.0.0.0"
  #       port: "8001"
  #       trainingServerUrl: "http://localhost:8000"
  #       localTtftModelPath: "/local_models/ttft.joblib"
  #       localTpotModelPath: "/local_models/tpot.joblib"
  #       localTtftScalerPath: "/local_models/ttft_scaler.joblib"
  #       localTpotScalerPath: "/local_models/tpot_scaler.joblib"
  #       httpTimeout: "30"
  #     env: []
  #     envFrom: []

inferencePool:
  targetPorts:
    - number: 8000
  modelServerType: vllm # vllm, triton-tensorrt-llm
  apiVersion: inference.networking.k8s.io/v1 
  # modelServers: # REQUIRED
  #   matchLabels:
  #     app: vllm-llama3-8b-instruct

  # Should only used if apiVersion is inference.networking.x-k8s.io/v1alpha2, 
  # This will soon be deprecated when upstream GW providers support v1, just doing something simple for now.
  targetPortNumber: 8000

provider:
  name: none

gke:
  monitoringSecret:
    name: inference-gateway-sa-metrics-reader-secret
    namespace: default

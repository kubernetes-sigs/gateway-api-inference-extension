inferenceExtension:
  replicas: 1
  image:
    name: epp
    hub: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension
    tag: main
    pullPolicy: Always
  extProcPort: 9002
  env: []
  pluginsConfigFile: "default-plugins.yaml"
  # Define additional container ports
  extraContainerPorts: []
  # Define additional service ports
  extraServicePorts: []

  # This is the plugins configuration file.
  # pluginsCustomConfig:
  #   custom-plugins.yaml: |
  #     apiVersion: inference.networking.x-k8s.io/v1alpha1
  #     kind: EndpointPickerConfig
  #     plugins:
  #     - type: custom-scorer
  #       parameters:
  #         custom-threshold: 64
  #     schedulingProfiles:
  #     - name: default
  #       plugins:
  #       - pluginRef: custom-scorer

  # Example environment variables:
  # env:
  #   ENABLE_EXPERIMENTAL_FEATURE: "true"

  flags:
    # Log verbosity
    - name: v
      value: 1

  affinity: {}

  tolerations: []

  # Monitoring configuration for EPP
  monitoring:
    interval: "10s"
    # Service account token secret for authentication
    secret:
      name: inference-gateway-sa-metrics-reader-secret

    # Prometheus ServiceMonitor will be created when enabled for EPP metrics collection
    prometheus:
      enabled: false
    
    gke:
      enabled: false

inferencePool:
  targetPorts:
    - number: 8000
  modelServerType: vllm # vllm, triton-tensorrt-llm
  apiVersion: inference.networking.k8s.io/v1 
  # modelServers: # REQUIRED
  #   matchLabels:
  #     app: vllm-llama3-8b-instruct

  # Should only used if apiVersion is inference.networking.x-k8s.io/v1alpha2, 
  # This will soon be deprecated when upstream GW providers support v1, just doing something simple for now.
  targetPortNumber: 8000

# Options: ["gke", "istio", "none"]
provider:
  name: none

  # GKE-specific configuration.
  # This block is only used if name is "gke".
  gke:
    # Set to true if the cluster is an Autopilot cluster.
    autopilot: false

istio:
  destinationRule:
    # Provide a way to override the default calculated host
    host: "" 
    # Optional: Enables customization of the traffic policy
    trafficPolicy: {}
      # connectionPool:
      #   http:
      #     maxRequestsPerConnection: 256000

opentelemetry:
  enabled: true
  # With this setting you can send trace to the exist opentelemetry collector based on opentelemetry-operator
  # See https://github.com/open-telemetry/opentelemetry-operator?tab=readme-ov-file#opentelemetry-auto-instrumentation-injection
  autoENVInject:
    # The possible values for the annotation can be
    # "true" - inject and Instrumentation resource from the namespace.
    # "my-instrumentation" - name of Instrumentation CR instance in the current namespace.
    # "my-other-namespace/my-instrumentation" - name and namespace of Instrumentation CR instance in
    # "false" - do not inject
    CRInstanceName: "false"
  # Add the required OTel environment manually
  # If you also enabled autoENVInject setting, the auto env inject will be skipped by opentelemetry-operator,
  env:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: "http://localhost:4317"
    - name: OTEL_SERVICE_NAME
      value: "gateway-api-inference-extension"
    - name: OTEL_RESOURCE_ATTRIBUTES_NODE_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.nodeName
    - name: OTEL_RESOURCE_ATTRIBUTES_POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: OTEL_RESOURCE_ATTRIBUTES
      value: 'k8s.namespace.name=$(NAMESPACE),k8s.node.name=$(OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=$(OTEL_RESOURCE_ATTRIBUTES_POD_NAME)'
# --- Test Job ---
apiVersion: batch/v1
kind: Job
metadata:
  name: latency-predictor-test
  namespace: default
  labels:
    app: latency-predictor-test
    component: test
spec:
  # Run multiple client pods in parallel to avoid client-side bottlenecks
  # Set to 1 for single-client testing, or 10 for distributed load
  parallelism: 10
  completions: 10

  template:
    metadata:
      labels:
        app: latency-predictor-test
        component: test
    spec:
      # Use the same node pool as your services
      # nodeSelector:
      #   cloud.google.com/gke-nodepool: "pool-2"
      restartPolicy: Never
      containers:
      - name: test-runner
        image: test-runner:latest
        imagePullPolicy: Always
        resources:
          requests:
            cpu: "500m"
            memory: "2Gi"  # Reduced since each pod handles 1/10th of load
          limits:
            cpu: "1000m"
            memory: "4Gi"
        env:
        # Point to your internal services
        - name: TRAINING_SERVER_URL
          value: "http://training-service:8000"
        - name: PREDICTION_SERVER_URL
          value: "http://prediction-service:80"
        - name: TEST_TIMEOUT
          value: "300"  # 5 minutes
        # Each pod runs 1/parallelism of total QPS (3000/10 = 300 per pod)
        - name: TARGET_QPS
          value: "300"  # Total across all pods: 300 * 10 = 3000 QPS
        - name: TARGET_QPS_LARGE_BATCH
          value: "30"   # Total across all pods: 30 * 10 = 300 QPS
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        envFrom:
        - configMapRef:
            name: prediction-server-config  # Reuse existing config if needed
        # Override the default command to run specific pytest
        # When running in parallel, skip training tests (they conflict across pods)
        # Only run stress tests that measure performance
        command: ["pytest"]
        args:
          - "-v"
          - "-s"
          - "test_dual_server_client.py"
          - "-k"
          - "stress"  # Only run *_stress_test functions
        # If your tests need to store results or logs
        volumeMounts:
        - name: test-results
          mountPath: /test-results
      volumes:
      - name: test-results
        emptyDir: {}
  backoffLimit: 3  # Retry up to 3 times if test fails

# --- Functional Test Job (Single Pod) ---
# Runs full test suite including model training validation
# Use this to verify the system is working correctly
apiVersion: batch/v1
kind: Job
metadata:
  name: latency-predictor-functional-test
  labels:
    app: latency-predictor-test
    component: functional-test
spec:
  template:
    metadata:
      labels:
        app: latency-predictor-test
        component: functional-test
    spec:
      restartPolicy: Never
      containers:
      - name: test-runner
        image: test-runner:latest
        imagePullPolicy: Always
        resources:
          requests:
            cpu: "1000m"
            memory: "4Gi"  # Increased for functional tests (training data generation)
          limits:
            cpu: "2000m"
            memory: "8Gi"
        env:
        # Point to your internal services
        - name: LOG_LEVEL
          value: DEBUG
        - name: TRAINING_SERVER_URL
          value: "http://training-service:8000"
        - name: PREDICTION_SERVER_URL
          value: "http://prediction-service:80"
        - name: TEST_TIMEOUT
          value: "300"  # 5 minutes
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        envFrom:
        - configMapRef:
            name: latency-predictor-shared-config
        - configMapRef:
            name: prediction-server-config
        # Run functional tests only (exclude stress tests)
        command: ["pytest"]
        args:
          - "-v"
          - "-s"
          - "test/test_dual_server_client.py"
          - "-k"
          - "not stress"  # Exclude all *_stress_test functions
        volumeMounts:
        - name: test-results
          mountPath: /test-results
      volumes:
      - name: test-results
        emptyDir: {}
  backoffLimit: 1

---
# --- Performance/Stress Test Job (Parallel Pods) ---
# Runs only stress tests across multiple pods to measure throughput
# Use this to test performance at high QPS without client-side bottlenecks

# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: latency-predictor-stress-test
#   labels:
#     app: latency-predictor-test
#     component: stress-test
# spec:
#   # Run multiple client pods in parallel to avoid client-side bottlenecks
#   parallelism: 10
#   completions: 10
#   completionMode: Indexed  # Give each pod a unique JOB_COMPLETION_INDEX
#   template:
#     metadata:
#       labels:
#         app: latency-predictor-test
#         component: stress-test
#     spec:
#       restartPolicy: Never
#       containers:
#       - name: test-runner
#         image: test-runner:latest
#         imagePullPolicy: Always
#         resources:
#           requests:
#             cpu: "2"       # Increased for JSON serialization/deserialization
#             memory: "3Gi"  # Sufficient for ~7000 responses (~2GB observed)
#           limits:
#             cpu: "4"       # Higher limit to avoid CPU throttling
#             memory: "6Gi"
#         env:
#         # Point to your internal services
#         - name: LOG_LEVEL
#           value: DEBUG
#         - name: TRAINING_SERVER_URL
#           value: "http://training-service:8000"
#         - name: PREDICTION_SERVER_URL
#           value: "http://prediction-service:80"
#         - name: TEST_TIMEOUT
#           value: "300"  # 5 minutes
#         # Each pod runs 1/parallelism of total QPS
#         - name: TARGET_QPS
#           value: "400"
#         - name: TARGET_QPS_LARGE_BATCH
#           value: "40"
#         # Set to "true" to run training warmup before stress tests
#         - name: RUN_TRAINING_WARMUP
#           value: "true"
#         - name: POD_NAME
#           valueFrom:
#             fieldRef:
#               fieldPath: metadata.name
#         envFrom:
#         - configMapRef:
#             name: latency-predictor-shared-config
#         - configMapRef:
#             name: prediction-server-config
#         # Run stress tests only (entrypoint handles warmup logic)
#         # Use args instead of command so ENTRYPOINT runs (test_entrypoint.sh)
#         args:
#           - "pytest"
#           - "-v"
#           - "-s"
#           - "test/test_dual_server_client.py"
#           - "-k"
#           - "stress"
#         volumeMounts:
#         - name: test-results
#           mountPath: /test-results
#       volumes:
#       - name: test-results
#         emptyDir: {}
#   backoffLimit: 1

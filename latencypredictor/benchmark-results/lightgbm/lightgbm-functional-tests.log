============================= test session starts ==============================
platform linux -- Python 3.9.25, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/bin/python3.9
cachedir: .pytest_cache
rootdir: /app
plugins: anyio-4.12.1, asyncio-1.2.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 30 items / 3 deselected / 27 selected

test_dual_server_client.py::test_prediction_server_healthz Waiting for prediction server...
Waiting for training server...
PASSED
test_dual_server_client.py::test_training_server_healthz PASSED
test_dual_server_client.py::test_prediction_server_readyz PASSED
test_dual_server_client.py::test_training_server_readyz PASSED
test_dual_server_client.py::test_prediction_server_status Prediction server using model type: lightgbm
Quantile: 0.9
Models ready: True
Models exist: {'ttft_model': True, 'tpot_model': True}
PASSED
test_dual_server_client.py::test_training_server_model_info Training server using model type: lightgbm
PASSED
test_dual_server_client.py::test_training_server_models_list Note: TreeLite is incompatible with quantile regression objectives
TreeLite models will be listed but may not exist when using quantile regression
Model ttft: exists=True, size=4748 bytes
Model tpot: exists=True, size=4644 bytes
PASSED
test_dual_server_client.py::test_model_download_from_training_server Successfully downloaded ttft model (4748 bytes)
Successfully downloaded tpot model (4644 bytes)
PASSED
test_dual_server_client.py::test_treelite_models_on_training_server Testing TreeLite models for lightgbm...
⚠️  Note: TreeLite doesn't support quantile regression objectives
⚠️  TreeLite models will NOT be available when using reg:quantileerror or objective=quantile
⚠️  TTFT TreeLite model not found (expected when using quantile regression)
⚠️  TPOT TreeLite model not found (expected when using quantile regression)
PASSED
test_dual_server_client.py::test_lightgbm_endpoints_on_training_server Testing LightGBM endpoints on training server...
✓ TTFT LightGBM text model available
✓ TPOT LightGBM text model available
✓ TTFT LightGBM importances available with 7 features
✓ TPOT LightGBM importances available with 5 features
PASSED
test_dual_server_client.py::test_add_training_data_to_training_server Successfully sent training data to training server
  Cleaned up test data: Successfully flushed: 43 TTFT and 43 TPOT training samples, 7 TTFT and 7 TPOT test samples, all metric scores
PASSED
test_dual_server_client.py::test_prediction_server_model_sync Model reload result: synced=False, loaded=True
Prediction server models are ready!
PASSED
test_dual_server_client.py::test_prediction_via_prediction_server Prediction successful: TTFT=10.00ms, TPOT=10.00ms
Model type: lightgbm
PASSED
test_dual_server_client.py::test_bulk_prediction_strict Testing bulk prediction strict endpoint...
✓ Bulk prediction strict endpoint test passed
PASSED
test_dual_server_client.py::test_bulk_prediction_with_validation_errors Testing bulk prediction validation error handling...
✓ Bulk prediction correctly failed when any request had validation errors
PASSED
test_dual_server_client.py::test_bulk_prediction_all_valid Testing bulk prediction with all valid requests...
✓ Bulk prediction succeeded with all valid requests
PASSED
test_dual_server_client.py::test_prediction_missing_prefix_cache_score ✓ Prediction correctly failed when prefix_cache_score was missing
PASSED
test_dual_server_client.py::test_training_server_metrics Training server metrics endpoint working correctly
✓ Prefix cache score feature found in metrics
PASSED
test_dual_server_client.py::test_model_consistency_between_servers Model type consistent across servers: lightgbm
PASSED
test_dual_server_client.py::test_model_specific_endpoints_on_training_server Testing LightGBM endpoints on training server...
✓ TTFT LightGBM text model available
✓ TPOT LightGBM text model available
✓ TTFT LightGBM importances available with 7 features
✓ TPOT LightGBM importances available with 5 features
PASSED
test_dual_server_client.py::test_dual_server_quantile_regression_learns_distribution Using random seed: 54286
Flushing old training data...
  Flushed old data: Successfully flushed: 0 TTFT and 0 TPOT training samples, 0 TTFT and 0 TPOT test samples, all metric scores
  WARNING: Background training added 2000 samples after flush verification
  Data clean: 0 samples (flush successful, verified stable)
Detected mode: native quantile (coverage tolerance: ±5.0%)
Final flush immediately before sending test data...
  Successfully flushed: 0 TTFT and 0 TPOT training samples, 0 TTFT and 0 TPOT test samples, all metric scores
  Pre-submission check: 0 samples (should be 0)
Submitting 5000 training samples...
  Attempt 1: POSTing 5000 entries to http://training-service:8000/add_training_data_bulk
  Response: 202 - {'message': 'Accepted 5000 training samples.'}
✓ Training data submitted successfully after 1 attempt(s)
DEBUG: Initial TTFT model hash: 374e3dd364a1ee1c23464d409863b308
Waiting for models to be retrained (initial last_load: 2026-01-14T14:45:32.008439Z)...
Note: Waiting for 2 training cycles to ensure new data is used
✓ First training cycle completed after 1 seconds (may be on stale data)
✓ Second training cycle completed after 1 more seconds (on new data)
✓ Models synced from training server after second training cycle (iteration 1)
DEBUG: Model hash: initial=374e3dd3, final=5b085a57, changed=True
Skipping conformal calibration check (native quantile mode doesn't use conformal prediction)
Waiting for all prediction server pods to sync good native quantile models...
  Attempt 1: Predictions some pods returning defaults (std: 151.62ms, monotonic: True) - waiting for good models...
✓ All pods synced after 3 seconds
  Avg predictions: input_len=100: 440.08ms, 400: 983.38ms, 600: 1363.85ms (std: 379.07)
Skipping calibration data verification (native quantile mode doesn't use conformal prediction)
DEBUG: Pre-prediction calibration check:
  use_treelite: False
  TTFT calibration samples: 0
  TPOT calibration samples: 0
  TTFT quantile adjustment: N/A
  TPOT quantile adjustment: N/A
Relative-err accuracy (≤15%): 96.8%
Coverage: TTFT=0.866, TPOT=0.866 (target 0.900 ± 0.05)
PASSED
test_dual_server_client.py::test_end_to_end_workflow Testing end-to-end workflow...
Step 1: Sending training data to training server...
Step 2: Waiting for training...
Step 3: Syncing models to prediction server...
Step 4: Making predictions...
  Prediction 1: TTFT=1342.53ms, TPOT=339.15ms (prefix_cache=0.75)
  Prediction 2: TTFT=423.90ms, TPOT=151.26ms (prefix_cache=0.55)
  Prediction 3: TTFT=431.04ms, TPOT=142.63ms (prefix_cache=0.04)
  Prediction 4: TTFT=1884.99ms, TPOT=544.47ms (prefix_cache=0.50)
  Prediction 5: TTFT=1883.30ms, TPOT=505.38ms (prefix_cache=0.73)
✓ End-to-end workflow completed successfully!
PASSED
test_dual_server_client.py::test_server_configuration Testing server configuration...
Prediction server: HTTP-based Quantile Latency Predictor is running
  Model type: lightgbm
  Is ready: True
  Sync interval: 10s
  Training server URL: http://training-service:8000
Training server: Latency Predictor is running.
  Model type: lightgbm
PASSED
test_dual_server_client.py::test_zzz_training_server_flush_api Testing training server flush API...
Step 1: Checking initial data status...
  Initial training samples: TTFT=4514, TPOT=4514
  Initial test samples: TTFT=506, TPOT=506
Step 2: Adding training data...
  Added 100 training samples
Step 3: Verifying data was added...
  After adding - Training: 9206, Test: 1034, Total: 10240
Step 4: Testing flush with only training data...
  Flushed 4603 TTFT training samples
  Flushed 4603 TPOT training samples
  Test samples flushed: 0 TTFT, 0 TPOT (should be 0)
  After training flush - Training: 0, Test: 1034
Step 5: Adding more training data...
Step 6: Testing flush everything...
  Complete flush message: Successfully flushed: 48 TTFT and 48 TPOT training samples, 519 TTFT and 519 TPOT test samples, all metric scores
  After complete flush - Training: 0, Test: 0
Step 7: Testing default flush (no body)...
  Default flush result: Successfully flushed: 18 TTFT and 18 TPOT training samples, 2 TTFT and 2 TPOT test samples, all metric scores
Step 8: Testing flush with only test data...
  Test data flush: 3 TTFT, 3 TPOT
  After test flush - Training: 94, Test: 0
Step 9: Testing bucket distribution in status...
  Bucket distribution available: 0 buckets with data
✓ Flush API tests passed!
PASSED
test_dual_server_client.py::test_training_server_flush_error_handling Testing flush API error handling...
✓ Invalid JSON handled correctly
✓ Flush error handling tests passed!
PASSED
test_dual_server_client.py::test_native_quantile_mode 
==================================================
Testing Native Quantile Mode (USE_TREELITE=false)
==================================================
Step 1: Checking server configuration...
  Model type: lightgbm
  Quantile: 0.9
Step 2: Verifying TreeLite models are NOT created...
  ✓ TreeLite models NOT created (as expected in native quantile mode)
Step 3: Sending training data...
  ✓ Sent 2000 training samples
Step 4: Waiting for training to complete...
Step 5: Syncing models to prediction server (waiting for 2 training cycles)...
  ✓ First training cycle after 1 seconds
  ✓ Second training cycle after 1 more seconds (on new data)
  Waiting for all prediction server pods to sync native quantile models...
  ✓ All pods synced after 1 seconds (std: 453.15ms)
Step 6: Testing predictions and coverage...
  Coverage: TTFT=89.0%, TPOT=86.0% (target: 90%)
✓ Native quantile mode test passed!
PASSED
test_dual_server_client.py::test_treelite_conformal_mode 
==================================================
Testing TreeLite + Conformal Mode (USE_TREELITE=true)
==================================================
Step 1: Checking server configuration...
  Model type: lightgbm
  Quantile: 0.9
Step 2: Verifying TreeLite models are created...
  ⚠️  TreeLite models NOT created - likely using native quantile mode
  This test expects USE_TREELITE=true
  TTFT TreeLite exists: False
  TPOT TreeLite exists: False
PASSED

====================== 27 passed, 3 deselected in 36.63s =======================
